{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der einfachen linearen Regression wird eine quantitative Zielvariable (response variable) $Y$ mit nur einer Prädiktorvariablen $X$ vorhergesagt. Mathematisch läßt sich das durch einen linearen Zusammenhang ausdrücken:\n",
    "\n",
    "$$Y \\approx \\beta_0 + \\beta_1 X$$\n",
    "\n",
    "Das Symbol $\\approx$ bedeutet \"wird näherungsweise modelliert als\".\n",
    "\n",
    "Die unbekannten Konstanten $\\beta_0, \\beta_1$ heißen **Parameter** oder **Koeffizienten** und repräsentieren den Ordinatenabstand (*intercept*) und die Steigung (*slope*) der Regressionsgeraden.\n",
    "\n",
    "Hat man mittels Trainingsdaten die Koeffizienten $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$ geschätzt, kann man zukünftige Werte für die Responsevariable mit folgender Gleichung vorhersagen:\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$$\n",
    "\n",
    "wobei $\\hat{y}$ anzeigt, dass es sich um einen geschätzten Wert handelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechnung der Koeffizienten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der linearen Regression wird versucht eine Gerade derart durch die Datenpunkte zu legen, dass der Abstand der Geraden zu den Datenpunkten minimal wird. Es gibt mehrere Methoden, ein Distanzmaß zu definieren, am häufigsten wird die Methode der kleinsten Quadrate genommen, daher nennt man diese Form der Regression auch **ordinary least squares regression (OLSR)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuen\n",
    "\n",
    "$\\hat{y}_i$ ist die Vorhersage der Variable $Y$ für den i-ten Wert von $X$:\n",
    "\n",
    "$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$$\n",
    "\n",
    "Der Wert\n",
    "\n",
    "$$e_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "ist das i-te **Residuum**, also der Unterschied des vorhergesagten zum beobachteten Wert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Sum of Squares (RSS)\n",
    "\n",
    "Die quadrierte Summe der Residuen bezeichnet man mit **RSS (Residual sum of squares)**:\n",
    "\n",
    "$$RSS = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i=1}^{n} e_i^2$$\n",
    "$$= \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "$$= \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimierung der RSS \n",
    "\n",
    "> Bei der Methode der kleinsten Quadrate werden $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$ so bestimmt, dass RSS minimal wird. \n",
    "\n",
    "Mathematisch gesehen wird die erste Ableitung Null gesetzt.\n",
    "\n",
    "$$\\frac{\\partial RSS}{\\partial \\hat{\\beta}_0} = \\sum_{i=1}^{n} -2 (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0$$\n",
    "\n",
    "$$\\frac{\\partial RSS}{\\partial \\hat{\\beta}_1} = \\sum_{i=1}^{n} -2 x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Koeffizienten\n",
    "\n",
    "Zunächst wird $\\hat{\\beta}_0$ bestimmt:\n",
    "\n",
    "$$0 = \\sum_{i=1}^{n} -2 (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)$$\n",
    "\n",
    "$$0= \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)$$\n",
    "\n",
    "$$0= \\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\hat{\\beta}_0 - \\sum_{i=1}^{n} \\hat{\\beta}_1 x_i$$\n",
    "\n",
    "$$0= n \\bar{y} - n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "$$0= \\bar{y} - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "> Der Wert des Koeffizienten $\\hat{\\beta}_0$ ist\n",
    "> \n",
    "> $$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "Um $\\hat{\\beta}_1$ zu bestimmen, setzen wir in Zeile 2 den eben ermittelten Wert für $\\hat{\\beta}_0$ ein:\n",
    "\n",
    "$$0 = \\sum_{i=1}^{n} -2 x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)$$\n",
    "\n",
    "$$= \\sum_{i=1}^{n} x_i y_i - \\hat{\\beta}_0 \\sum_{i=1}^{n} x_i - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2$$\n",
    "\n",
    "$$= \\sum_{i=1}^{n} x_i y_i - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum_{i=1}^{n} x_i - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2$$\n",
    "\n",
    "$$= \\sum_{i=1}^{n} x_i (y_i - \\bar{y}) - \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 - \\hat{\\beta}_1 \\bar{x}\\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "$$= \\sum_{i=1}^{n} x_i (y_i - \\bar{y}) - \\hat{\\beta}_1 \\sum_{i=1}^{n}x_i(x_i - \\bar{x})$$\n",
    "\n",
    "und somit\n",
    "\n",
    "> Der Wert des Koeffizienten $\\hat{\\beta}_1$ ist\n",
    "> \n",
    "> $$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} x_i (y_i - \\bar{y})}{\\sum_{i=1}^{n}x_i(x_i - \\bar{x})}$$\n",
    "\n",
    "Den Regressionsparameter $\\hat{\\beta}_1$ findet man in der Literatur meist in drei verschiedenen Darstellungen. Eine weitere gängige Darstellung ergibt sich, in dem man die Summen aufspaltet und die Mittelwerte einsetzt:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} x_i y_i - \\frac{1}{n}\\sum_{i=1}^{n}x_i \\sum_{i=1}^{n}y_i}{\\sum_{i=1}^{n}x_i^2 - \\frac{1}{n}(\\sum_{i=1}^{n} x_i)^2}\n",
    "$$\n",
    "\n",
    "Die wohl bekannteste Darstellung ergibt sich, in dem man die Summenregeln für Mittelwerte anwendet:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Überprüfung der Minimalität\n",
    "\n",
    "Es gilt noch zu überprüfen, ob es sich bei den gefundenen Werten tatsächlich um Minima handelt. Dazu sieht man sich im zweidimensionalen Fall zunächst die zweiten partiellen Ableitungen an:\n",
    "\n",
    "$$\\frac{\\partial^2 RSS}{\\partial \\hat{\\beta}_0^2} = -2 \\sum_{i=1}^{n} (-1) = 2n$$\n",
    "\n",
    "$$\\frac{\\partial^2 RSS}{\\partial \\hat{\\beta}_1^2} = 2 \\sum_{i=1}^{n}x_i^2$$\n",
    "\n",
    "$$\\frac{\\partial^2 RSS}{\\partial \\hat{\\beta}_0\\partial\\hat{\\beta}_1} = 2 \\sum_{i=1}^{n}x_i = 2n\\bar{x}$$\n",
    "\n",
    "Die entsprechende Hesse-Matrix ist:\n",
    "\n",
    "$$\n",
    "H =  \n",
    " \\left (\n",
    " {\\begin{array}{cc}\n",
    "   2n & 2n\\bar{x} \\\\\n",
    "   2n\\bar{x} & 2 \\sum_{i=1}^{n}x_i^2 \\\\\n",
    "  \\end{array} } \n",
    "  \\right )\n",
    "$$\n",
    "\n",
    "Damit ein Minimum vorliegt, muss die Hesse-Matrix positiv definit sein. Das wird nun auf zwei Arten überprüft. \n",
    "\n",
    "Sind die Determinanten der Hauptminoren positiv? \n",
    "\n",
    "$$\\det H_1 = 2n > 0$$\n",
    "\n",
    "$$\\det H_2 = \\det H = 4n \\sum x_i^2 - 4 n^2 \\bar{x}^2 > 0$$\n",
    "\n",
    "$$\\sum x_i^2 - n \\bar{x}^2 > 0$$\n",
    "\n",
    "$$\\sum x_i^2 - \\frac{\\left (\\sum x_i \\right )^2}{n} > 0$$\n",
    "\n",
    "$$n \\sum x_i^2 > \\left (\\sum x_i \\right)^2$$\n",
    "\n",
    "Mit Hilfe der Cauchy-Schwarz Ungleichung mit $a_i = x_i$ und $b_i = 1$ folgt:\n",
    "\n",
    "$$\\left (\\sum x_i \\right )^2 = \\left (\\sum x_i \\cdot 1 \\right )^2 \\leq \\sum x_i^2 \\cdot \\sum 1^2 = n \\sum x_i^2$$\n",
    "\n",
    "Da Gleichheit nur gilt wenn alle $x_i$ gleich sind ist die positive Definitheit bewiesen.\n",
    "\n",
    "Alternativ kann man sich die Definition einer positiv definiten Matrix ansehen und mit einem beliebigen zweidimensionalen (Spalten-)Vektor $v$ testen ob $v^T H v > 0$ ist:\n",
    "\n",
    "$$\n",
    "\\left (\n",
    "\\begin{array}{cc}\n",
    "   a & b \n",
    "\\end{array}\n",
    "\\right )\n",
    "\\cdot\n",
    "\\left (\n",
    "\\begin{array}{cc}\n",
    "   2n & 2n\\bar{x} \\\\\n",
    "   2n\\bar{x} & 2 \\sum_{i=1}^{n}x_i^2 \\\\\n",
    "\\end{array}\n",
    "\\right )\n",
    "\\cdot\n",
    "\\left (\n",
    "\\begin{array}{c}\n",
    "   a \\\\\n",
    "   b\\\\\n",
    "  \\end{array}\n",
    "\\right ) > 0\n",
    "$$\n",
    "\n",
    "Zunächst wird $v^T$ mit $H$ multipliziert. Das Produkt eines $1 \\times 2$ Vektors mit einer $2 \\times 2$ Matrix liefert einen $1 \\times 2$ Vektor:\n",
    "\n",
    "$$\n",
    "\\left (\n",
    "\\begin{array}{cc}\n",
    "   2an + 2nb\\bar{x} & 2na\\bar{x} + 2b \\sum x_i^2\n",
    "\\end{array}\n",
    "\\right )\n",
    "\\cdot\n",
    "\\left (\n",
    "\\begin{array}{c}\n",
    "   a \\\\\n",
    "   b\\\\\n",
    "  \\end{array}\n",
    "\\right ) > 0\n",
    "$$\n",
    "\n",
    "Nun wird ein $1 \\times 2$ Vektor mit einem $2 \\times 1$ Vektor multipliziert, das Ergebnis ist demnach eine Zahl ($1 \\times 1$):\n",
    "\n",
    "$$a \\cdot \\left ( 2na + 2nb\\bar{x} \\right ) + b\\cdot \\left ( 2na\\bar{x} + 2b\\sum x_i^2 \\right) > 0$$\n",
    "\n",
    "$$na^2 + 2nab\\bar{x} + b\\sum x_i^2 > 0$$\n",
    "\n",
    "$$\\sum a^2 + 2ab\\sum x_i + 2b\\sum x_i^2 > 0$$\n",
    "\n",
    "$$\\sum \\left (a^2 + 2ab x_i + b^2 x_i^2 \\right ) > 0$$\n",
    "\n",
    "$$\\sum (a + bx_i)^2 > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genauigkeit des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardfehler und Residual Standard Error (RSE)\n",
    "\n",
    "Die **Standardfehler der Koeffizienten** $\\hat{\\beta}_0, \\hat{\\beta}_1$ geben an, wie nahe die geschätzten Koeffizienten bei den tatsächlichen Werten von $\\beta_0$ und $\\beta_1$ liegen. Es gelten folgende Formeln: \n",
    "\n",
    "$$SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\cdot \\left ( \\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\right )$$\n",
    "\n",
    "$$SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "\n",
    "Das $\\sigma^2$ ist die Varianz des Fehlerterms, die allerdings unbekannt ist. Die Abschätzung dafür nennt man **Residual Standard Error (RSE)**:\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n-p-1}} = \\sqrt{\\frac{1}{n-p-1}\\sum_{i=1}^n \\left (y_i - \\hat{y}_i \\right )^2}$$\n",
    "\n",
    "wobei $p$ die Anzahl der Prädiktoren ist (in unserem Fall der simplen Regression ist $p=1$). Der RSE ist eine Abschätzung der Standardabweichung von $\\epsilon$, oder grob gesagt die durchschnittliche Abweichung von der wahren Regressionslinie. \n",
    "\n",
    "Das RSE ist somit ein Maß wie gut das Model an die Daten angepasst wurde, denn ist $\\hat{y}_i \\approx y_i \\forall i=1,\\dots,n$, dann wird das RSE sehr klein. \n",
    "\n",
    "> **Je kleiner** RSE, **desto besser** ist das Model an die Daten angepasst. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error (MSE)\n",
    "\n",
    "Ein sehr häufig verwendetes Maß bei Regressionen ist der **Mean Squared Error (MSE)**, der durch \n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\left ( y_i - \\hat{y}_i \\right )^2 = \\frac{RSS}{n}$$\n",
    "\n",
    "gegeben ist. \n",
    "\n",
    "> Ein **kleiner** MSE bedeutet, dass die vorhergesagten Werte nahe bei den tatsächlichen Werten liegen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konfidenzintervall\n",
    "\n",
    "Mit Hilfe der Standardfehler der Koeffizienten können wir nun Konfidenzintervalle berechnen. Bekanntlich sagt uns ein 95% Konfidenzintervall, dass der wahre Wert der unbekannten Parameter mit 95% Wahrscheinlichkeit in dem Bereich des Konfidenzintervalls liegen.\n",
    "\n",
    "Näherungsweise gilt für die Koeffizienten $\\hat{\\beta}_i \\, (i=0,1)$: \n",
    "\n",
    "$$CI = \\left [ \\hat{\\beta}_i - 2\\cdot SE(\\hat{\\beta_i}), \\hat{\\beta}_i + 2\\cdot SE(\\hat{\\beta_i})\\right ]$$\n",
    "\n",
    "Der Faktor 2 gilt nur als Approximation, für ein genaues Ergebnis benötigt man den Wert der t-Verteilung für $n-2 = 6$ Freiheitsgrade und 95% Konfidenz. Ein Blick in eine Tabelle liefert 2.4469."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesentests\n",
    "\n",
    "Die Standardfehler ermöglichen es uns auch, *Hypothesentests* für die Koeffizienten durchzuführen, da die Teststatistik \n",
    "\n",
    "$$\\frac{\\hat{\\beta}_i - \\beta_{(H_0)}}{SE(\\hat{\\beta}_i)}$$\n",
    "\n",
    "einer t-Verteilung mit $n-p$ Freiheitsgraden folgt. Dabei ist $\\beta_{(H_0)}$ der Regressionskoeffizient, der in der Nullhypothese postuliert wird (meistens 0).\n",
    "\n",
    "Die gängiste Variante eines Hypothesentests für die Regression ist (mit der Nullhypothese $H_0$ und der Alternativhypothese $H_1$):\n",
    "\n",
    "$$H_0: \\beta_1 = 0$$\n",
    "\n",
    "$$H_1: \\beta_1 \\neq 0$$\n",
    "\n",
    "Um die Nullhypothese zu testen berechnen wir eine t-Statistik (auch bekannt als Wald-Test)\n",
    "\n",
    "$$t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}$$\n",
    "\n",
    "die angibt, wie viele Standardabweichungen $\\hat{\\beta}_1$ von 0 entfernt ist. Ergibt unser Test bei einem Signifikanzniveau von $\\alpha = 0.05$ ein $p < 0,05$, dann wird $H_0$ verworfen (erinnere: *if p is low, $H_0$ must go*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$\n",
    "\n",
    "Der RSE ist in Einheiten von $Y$ gemessen. Um ein skalenunabhängiges Maß zu haben verwendet man eine Verhältnisgröße der erklärten Varianz, die zwischen 0 und 1 liegt. \n",
    "\n",
    "Bezeichnen wir zunächst mit **TSS** (*total sum of squares*) die gesamte Streuung der Daten (Varianz):\n",
    "\n",
    "$$TSS = \\sum_{i=1}^n \\left ( y_i - \\bar{y} \\right )^2$$\n",
    "\n",
    "Dann lässt sich diese in Analogie zur Varianzanalyse zerlegen (mit $\\bar{y} = (1/n)\\sum \\hat{y}_i$):\n",
    "\n",
    "$$\\sum_{i=1}^n \\left ( y_i - \\bar{y} \\right )^2 = \\sum_{i=1}^n \\left ( \\hat{y}_i - \\bar{y} \\right )^2 + \\sum_{i=1}^n \\left ( y_i - \\hat{y}_i \\right )^2$$\n",
    "\n",
    "oder anders gesagt: Die gesamte Varianz ist die Varianz der Schätzwerte plus der Varianz der Residuen. Das gesuchte Maß ist dann: \n",
    "\n",
    "$$R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}$$\n",
    "\n",
    "> Ein Wert von $R^2$ nahe 1 bedeutet, dass ein großer Teil der Variabilität der Response durch die Regression erklärt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted $R^2$\n",
    "\n",
    "Bei Modellen mit mehreren Variablen wird das $R^2$ angepasst, um die Güte von Modellen mit unterschiedlicher Anzahl an ausgewählten Variablen bewerten zu können - mehr dazu später. An dieser Stelle wollen wir uns nur die Formel dafür ansehen, wobei $d$ die Anzahl der gewählten Variablen ist (also $d=1$):\n",
    "\n",
    "$$R_{adj}^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)} = 1 - \\frac{(n-1)\\cdot RSS}{(n-2)\\cdot TSS}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Statistik\n",
    "\n",
    "Die F-Statistik spielt für Hypothesentests bei der multiplen linearen Regression eine Rolle. An dieser Stelle sei nur die Formel dafür erwähnt:\n",
    "\n",
    "$$F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)} = (n-2)\\frac{TSS-RSS}{RSS}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood\n",
    "\n",
    "Die Likelihood-Funktion für die lineare Regression ist (unabhängige Beobachtungen vorausgesetzt) das Produkt der Wahrscheinlichkeitsdichten für die $n$ Beobachtungen:\n",
    "\n",
    "$$L = \\prod_{i=1}^{n} \\frac{1}{\\sigma\\sqrt{2\\pi}} exp \\left (- \\frac{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}{2 \\sigma^2} \\right )=$$\n",
    "\n",
    "$$= \\left ( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right )^n exp \\left (- \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\right )=$$\n",
    "\n",
    "$$= \\left ( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right )^n e^{-RSS/2\\sigma^2}=$$\n",
    "\n",
    "$$= (2\\pi\\sigma^2)^{-n/2} \\cdot e^{-RSS/2\\sigma^2}$$\n",
    "\n",
    "Die Log-Likelihood-Funktion für die lineare Regression ist daher (mit $RSS = \\sigma^2 \\cdot n$): \n",
    "\n",
    "$$LL = \\ln(L) = \\ln ( (2\\pi\\sigma^2)^{-n/2} \\cdot e^{-RSS/2\\sigma^2}) = $$\n",
    "\n",
    "$$= \\ln((2\\pi\\sigma^2)^{-n/2}) - \\left ( \\frac{RSS}{2\\sigma^2} \\right ) = $$\n",
    "\n",
    "$$= -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{n\\sigma^2}{2\\sigma^2} = $$\n",
    "\n",
    "$$= -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{n}{2} = $$\n",
    "\n",
    "$$= -\\frac{n}{2}\\ln(2\\pi) -\\frac{n}{2}\\ln \\left (\\frac{RSS}{n}\\right ) -\\frac{n}{2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC, BIC\n",
    "\n",
    "Das Akaike Information Criterion (AIC) und das Bayesian Information Criterion (BIC) sind definiert als: \n",
    "\n",
    "$$AIC = 2k - 2\\cdot LL$$\n",
    "\n",
    "$$BIC = \\ln(n)\\cdot k - 2\\cdot LL$$\n",
    "\n",
    "mit $k$ der Anzahl der zu schätzenden Parameter (also in unserem Fall mit $\\beta_0, \\beta_1, \\epsilon$ ist $k=3$).\n",
    "\n",
    "AIC und BIC werden verwendet, um verschiedene Modelle miteinander zu vergleichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein einfaches Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from bdsm import quality\n",
    "from bdsm.datasets import sunshine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz laden\n",
    "df = sunshine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe has 8 rows and 2 columns.\n",
      "\n",
      "0 columns have missing values.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                    type  unique  missing_abs  missing_rel\nSonnenstunden    float64       8            0          0.0\nKonzertbesucher    int64       7            0          0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>unique</th>\n      <th>missing_abs</th>\n      <th>missing_rel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sonnenstunden</th>\n      <td>float64</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Konzertbesucher</th>\n      <td>int64</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineare Regression händisch berechnen\n",
    "\n",
    "**Response- und Predictorvariable festlegen**\n",
    "\n",
    "Wir speichern die Werte der Predictorvariablen $X$ in $x$ und die der Responsevariablen $Y$ in $y$ und führen einige Abkürzungen zur besseren Lesbarkeit ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df.Sonnenstunden)\n",
    "y = np.array(df.Konzertbesucher)\n",
    "\n",
    "# Nützliche Abkürzungen\n",
    "x_bar = np.mean(x)\n",
    "y_bar = np.mean(y)\n",
    "SXX = np.sum((x-x_bar)**2)\n",
    "SXY = np.sum((x-x_bar)*(y-y_bar))\n",
    "\n",
    "# Anzahl der Datenpunkte\n",
    "n = np.size(x)\n",
    "\n",
    "# Anzahl der Prädiktoren\n",
    "p = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berechnung der Koeffizienten\n",
    "\n",
    "In der neuen Schreibweise berechnet sich der Regressionskoeffizient $\\hat{\\beta}_1$ als\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{SXY}{SXX}$$\n",
    "\n",
    "Somit erhalten wir für die Regressionskoeffizienten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat_1 = SXY/SXX\n",
    "beta_hat_0 = y_bar - beta_hat_1 * x_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "5.336410534890034"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "15.728319304914475"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuen\n",
    "\n",
    "Mit den Werten für $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$ können wir die Prädiktoren $\\hat{y}$ bestimmen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = beta_hat_1 * x + beta_hat_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daraus ergeben sich sofort die Residuen $e_i$, die wir in der Variable `res` speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-3.86749932,  3.93065436, -2.80483302,  5.99332066, -2.80944882,\n        3.92142275, -5.21314146,  0.84952484])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Residuen\n",
    "res = y - y_hat\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardfehler\n",
    "\n",
    "Für die Berechnung der Standardfehler (Koeffizienten und Residuen) benötigen wie die RSS sowie $\\sigma^2$, welches wir in `sigma_sq` speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSE:  4.570989515784033\n",
      "SE(beta_0):  4.437227000146887  SE(beta_1):  0.9527289390578388\n"
     ]
    }
   ],
   "source": [
    "# Residual Sum of Squares (RSS)\n",
    "RSS = np.sum(res**2)\n",
    "\n",
    "# Sigma squared\n",
    "sigma_sq = RSS/(n-p-1)\n",
    "\n",
    "# Residual Standard Error (RSE)\n",
    "RSE = np.sqrt(sigma_sq)\n",
    "print(\"RSE: \", RSE)\n",
    "\n",
    "# Standard Error of Regression Coefficients\n",
    "se_beta_0 = np.sqrt(sigma_sq * (1/n + (x_bar**2)/SXX))\n",
    "se_beta_1 = np.sqrt(sigma_sq/SXX)\n",
    "print(\"SE(beta_0): \", se_beta_0, \" SE(beta_1): \", se_beta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 15.670458865055664\n"
     ]
    }
   ],
   "source": [
    "MSE = RSS/n\n",
    "print(\"MSE:\", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Konfidenzintervall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI_approx:\n",
      " [[ 6.8538653  24.60277331]\n",
      " [ 3.43095266  7.24186841]]\n",
      "CI:\n",
      " [[ 4.87081598 26.58582263]\n",
      " [ 3.00516681  7.66765426]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Näherungsweise\n",
    "CI_approx = np.array([[beta_hat_0 - 2*se_beta_0, beta_hat_0 + 2*se_beta_0],[beta_hat_1 - 2*se_beta_1, beta_hat_1 + 2*se_beta_1]])\n",
    "\n",
    "# Den Wert für die t-Verteilung mit df=6 bekommt man auch direkt aus Python\n",
    "studT = stats.t.ppf(1-0.025,6)\n",
    "\n",
    "CI = np.array([[beta_hat_0 - studT*se_beta_0, beta_hat_0 + studT*se_beta_0],[beta_hat_1 - studT*se_beta_1, beta_hat_1 + studT*se_beta_1]])\n",
    "\n",
    "print(\"CI_approx:\\n\", CI_approx)\n",
    "print(\"CI:\\n\", CI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesentests\n",
    "\n",
    "Wir berechnen die t-Statistik für beide Regressionskoeffizienten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_beta_0:  3.544628053600552 \n",
      "t_beta_1:  5.6011844671867035\n"
     ]
    }
   ],
   "source": [
    "# t-Values\n",
    "t_beta_1 = beta_hat_1/se_beta_1\n",
    "t_beta_0 = beta_hat_0/se_beta_0\n",
    "print(\"t_beta_0: \", t_beta_0, \"\\nt_beta_1: \", t_beta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dazu interessieren uns jetzt die p-Werte, zu deren Berechnung Python einen Näherungsalgorithmus verwendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012150695888024199\n"
     ]
    }
   ],
   "source": [
    "t = 3.54462805 # t-Value\n",
    "f = 1.0\n",
    "tz = 1.0\n",
    "j = 2.0 # Zählvariable\n",
    "k = 6.0 # Anzahl der Freiheitsgrade\n",
    "\n",
    "z = 1 + (t**2)/k\n",
    "\n",
    "while (j <= (k-2)) :\n",
    "    tz = tz * ((j-1)/(z*j))\n",
    "    f = f + tz\n",
    "    j = j + 2\n",
    "    \n",
    "p = f * t / np.sqrt(z * k)\n",
    "p = 0.5 + 0.5*p\n",
    "p = 2*(1-p)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht sofort, dass die `while` Schleife in unserem einfachen Fall nur zweimal durchlaufen wird. Daher gilt für die Variablen $tz$ und $f$ mit $tz_0 = 1, j_0 = 2$:\n",
    "\n",
    "$$tz_1 = tz_0 \\frac{j_0 - 1}{zj_0} = \\frac{1}{2z}$$\n",
    "$$tz_2 = tz_1 \\frac{j_i - 1}{zj_1} = \\frac{1}{2z}\\frac{3}{4z} = \\frac{3}{8z^2}$$\n",
    "$$f_1 = f_0 + tz_1 $$\n",
    "$$f_2 = f_1 + tz_2 = f_0 + tz_1 + tz_2 = 1 + \\frac{1}{2z} + \\frac{3}{8z^2}$$\n",
    "\n",
    "Somit folgt nach einer kleinen Umformung für die p-Werte:\n",
    "\n",
    "$$p = 1 - \\Big( 1 + \\frac{1}{2z} + \\frac{3}{8z^2} \\Big) \\frac{t}{\\sqrt{6z}}$$\n",
    "\n",
    "Eine Formel, die sich mit jedem besseren Taschenrechner ausrechnen läßt! Das wollen wir gleich testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.012150695835125958"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t_beta_0\n",
    "z = 1 + (t**2)/6\n",
    "\n",
    "p_val = 1 - (1 + 1/(2*z) + 3/(8*(z**2)))*t/np.sqrt(6*z)\n",
    "p_beta_0 = p_val\n",
    "p_beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.001379362627713876"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t_beta_1\n",
    "z = 1 + (t**2)/6\n",
    "\n",
    "p_val = 1 - (1 + 1/(2*z) + 3/(8*(z**2)))*t/np.sqrt(6*z)\n",
    "p_beta_1 = p_val\n",
    "p_beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8394574407934108"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total sum of Squares\n",
    "TSS = np.sum((y-y_bar)**2)\n",
    "\n",
    "# R^2\n",
    "R_sq = 1 - RSS/TSS\n",
    "R_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8127003475923127"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusted R^2\n",
    "R_sq_adj = 1 - ((n-1)*RSS)/((n-2)*TSS)\n",
    "R_sq_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-Statistik\n",
    "\n",
    "Leider gibt es für den p-Wert der F-Statistik keine einfache Näherungsformel, sondern nur eine komplizierte Iteration von Gamma- und Betafunktionen, deren Herleitung den Rahmen dieser Lehrveranstaltung übersteigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "31.373267435453595"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F-Statistic\n",
    "F_stat = (n-2)*((TSS-RSS)/RSS)\n",
    "F_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "-22.358617621507904"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogLik = -(n/2)*np.log(2*np.pi) - (n/2)*np.log(RSS/n)-n/2\n",
    "LogLik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIC, BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC:  50.71723524301581 \n",
      "BIC:  50.95555986805532\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "AIC = 2*k - 2*LogLik\n",
    "BIC = np.log(n)*k - 2*LogLik\n",
    "print(\"AIC: \", AIC, \"\\nBIC: \", BIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "97.609375"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Squared Error (MSE)\n",
    "MSE = TSS/n\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression in Python mit sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response- und Predictorvariable festlegen\n",
    "\n",
    "`sklearn` benötigt die Predictorvariable(n) $X$ und die Responsevariable $Y$ als Spaltenvektoren. Dazu verwenden wir die `reshape` Methode von numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.Sonnenstunden).reshape((-1, 1))\n",
    "Y = np.array(df.Konzertbesucher).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[22],\n       [33],\n       [30],\n       [42],\n       [38],\n       [49],\n       [42],\n       [55]], dtype=int64)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berechnung der Koeffizienten\n",
    "\n",
    "Die eigentliche Regression wird mit der Methode `fit()` durchgeführt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehen wir uns die Koeffizienten der Regression an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$\\hat{\\beta}_0$:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle 15.728319304914468$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$\\hat{\\beta}_1$:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle 5.336410534890036$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "beta_0 = model.intercept_[0]\n",
    "beta_1 = model.coef_[0][0]\n",
    "\n",
    "display(Latex(r'$\\hat{\\beta}_0$:')) \n",
    "display(Math(r'{}'.format(beta_0)))\n",
    "display(Latex(r'$\\hat{\\beta}_1$:'))\n",
    "display(Math(r'{}'.format(beta_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Regressionsgleichung ist somit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle Y = 15.73 + 5.34 \\cdot X$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'Y = {} + {} \\cdot X'.format(round(beta_0,2),round(beta_1,2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$\n",
    "\n",
    "Nun wollen wir uns die Responsevariable $\\hat{Y}$ vorhersagen lassen und in `Y_hat` speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das $R^2$ bekommen wir entweder direkt aus `model`oder mittels `metrics.r2_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8394574407934109"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8394574407934109"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.r2_score(Y, Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "15.67045886505566"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(Y,Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression in Python mit statsmodels\n",
    "\n",
    "Eine bequemere Art die lineare Regression durchzuführen ist das Paket `statsmodels` zu verwenden, wie schon die wesentlich kompaktere und übersichtlichere Syntax zeigt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('Konzertbesucher ~ Sonnenstunden', data=df).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels` bietet auch eine nette Übersicht der Parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        Konzertbesucher   R-squared:                       0.839\n",
      "Model:                            OLS   Adj. R-squared:                  0.813\n",
      "Method:                 Least Squares   F-statistic:                     31.37\n",
      "Date:                Mon, 08 Nov 2021   Prob (F-statistic):            0.00138\n",
      "Time:                        15:44:02   Log-Likelihood:                -22.359\n",
      "No. Observations:                   8   AIC:                             48.72\n",
      "Df Residuals:                       6   BIC:                             48.88\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept        15.7283      4.437      3.545      0.012       4.871      26.586\n",
      "Sonnenstunden     5.3364      0.953      5.601      0.001       3.005       7.668\n",
      "==============================================================================\n",
      "Omnibus:                        2.454   Durbin-Watson:                   3.403\n",
      "Prob(Omnibus):                  0.293   Jarque-Bera (JB):                0.846\n",
      "Skew:                           0.188   Prob(JB):                        0.655\n",
      "Kurtosis:                       1.452   Cond. No.                         13.3\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apigl\\.virtualenvs\\bdsm-Grc_FbtC\\lib\\site-packages\\scipy\\stats\\stats.py:1541: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen uns jetzt die von uns berechneten Größen im Vergleich zur Ausgabe von `statsmodel` an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value\t\tStatsmodel\t\tManual\t\t\tDelta\n",
      "beta_0\t\t 15.728319304914468 \t 15.728319304914475 \t 7.105427357601002e-15\n",
      "beta_1\t\t 5.3364105348900335 \t 5.336410534890034 \t 8.881784197001252e-16\n",
      "t beta_0\t 3.5446280536005523 \t 3.544628053600552 \t 4.440892098500626e-16\n",
      "t beta_1\t 5.601184467186704 \t 5.6011844671867035 \t 8.881784197001252e-16\n",
      "p beta_0\t 0.012150695835125904 \t 0.012150695835125958 \t 5.377642775528102e-17\n",
      "p beta_1\t 0.001379362627713965 \t 0.001379362627713876 \t 8.912141857830846e-17\n",
      "CI beta0 lo\t 4.870815982476692 \t 4.870815982476692 \t 0.0\n",
      "CI beta0 hi\t 26.585822627352243 \t 26.585822627352258 \t 1.4210854715202004e-14\n",
      "CI beta1 lo\t 3.0051668052226814 \t 3.0051668052226814 \t 0.0\n",
      "CI beta1 hi\t 7.6676542645573855 \t 7.667654264557387 \t 1.7763568394002505e-15\n",
      "R^2\t\t 0.8394574407934108 \t 0.8394574407934108 \t 0.0\n",
      "adj. R^2\t 0.8127003475923126 \t 0.8127003475923127 \t 1.1102230246251565e-16\n",
      "F-Stat\t\t 31.373267435453595 \t 31.373267435453595 \t 0.0\n",
      "LogLik\t\t -22.358617621507904 \t -22.358617621507904 \t 0.0\n",
      "AIC\t\t 48.71723524301581 \t 50.71723524301581 \t 2.0\n",
      "BIC\t\t 48.876118326375476 \t 50.95555986805532 \t 2.0794415416798415\n"
     ]
    }
   ],
   "source": [
    "print(\"Value\\t\\tStatsmodel\\t\\tManual\\t\\t\\tDelta\")\n",
    "print(\"beta_0\\t\\t\", results.params[0],\"\\t\",beta_hat_0,\"\\t\", abs(results.params[0]-beta_hat_0))\n",
    "print(\"beta_1\\t\\t\", results.params[1], \"\\t\", beta_hat_1,\"\\t\", abs(results.params[1]-beta_hat_1))\n",
    "print(\"t beta_0\\t\", results.tvalues[0], \"\\t\", t_beta_0, \"\\t\", abs(results.tvalues[0]-t_beta_0))\n",
    "print(\"t beta_1\\t\", results.tvalues[1], \"\\t\", t_beta_1, \"\\t\", abs(results.tvalues[1]-t_beta_1))\n",
    "print(\"p beta_0\\t\", results.pvalues[0], \"\\t\", p_beta_0, \"\\t\", abs(results.pvalues[0]-p_beta_0))\n",
    "print(\"p beta_1\\t\", results.pvalues[1], \"\\t\", p_beta_1, \"\\t\", abs(results.pvalues[1]-p_beta_1))\n",
    "print(\"CI beta0 lo\\t\", results.conf_int().values[0][0], \"\\t\", CI[0][0], \"\\t\", abs(results.conf_int().values[0][0]-CI[0][0]))\n",
    "print(\"CI beta0 hi\\t\", results.conf_int().values[0][1], \"\\t\", CI[0][1], \"\\t\", abs(results.conf_int().values[0][1]-CI[0][1]))\n",
    "print(\"CI beta1 lo\\t\", results.conf_int().values[1][0], \"\\t\", CI[1][0], \"\\t\", abs(results.conf_int().values[1][0]-CI[1][0]))\n",
    "print(\"CI beta1 hi\\t\", results.conf_int().values[1][1], \"\\t\", CI[1][1], \"\\t\", abs(results.conf_int().values[1][1]-CI[1][1]))\n",
    "print(\"R^2\\t\\t\", results.rsquared, \"\\t\", R_sq, \"\\t\", abs(results.rsquared - R_sq))\n",
    "print(\"adj. R^2\\t\", results.rsquared_adj, \"\\t\", R_sq_adj, \"\\t\", abs(results.rsquared_adj - R_sq_adj))\n",
    "print(\"F-Stat\\t\\t\", results.fvalue, \"\\t\", F_stat, \"\\t\", abs(results.fvalue - F_stat))\n",
    "print(\"LogLik\\t\\t\", results.llf, \"\\t\", LogLik, \"\\t\", abs(results.llf - LogLik))\n",
    "print(\"AIC\\t\\t\", results.aic, \"\\t\", AIC,\"\\t\", abs(results.aic-AIC))\n",
    "print(\"BIC\\t\\t\", results.bic, \"\\t\", BIC,\"\\t\", abs(results.bic-BIC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etwas später werden wir die Differenz bei AIC und BIC erklären."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemeinsamkeiten und Unterschiede zu R\n",
    "\n",
    "Wir sehen uns nun die lineare Regression in R an:\n",
    "\n",
    "<a href=\"http://rstudio.bds.fhstp.ac.at\" target=\"new\">R Studio Server Pro</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuen\n",
    "\n",
    "Sehen wir uns an, wie Python standardmässig die Quartile der Residuen ausgibt. Dazu berechnen wir sie zunächst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-5.21314146, -3.86749932, -2.80944882, -2.80483302,  0.84952484,\n        3.92142275,  3.93065436,  5.99332066])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals = y - y_hat\n",
    "residuals.sort()\n",
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-5.21314146, -3.07396144, -0.97765409,  3.92373065,  5.99332066])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(residuals,[.0,.25,.5,.75,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die standardmässige Ausgabe liefert das gleiche Ergebnis wie der Standard bei R. Aber auch in Python gibt es mehrere Optionen. Die Option `type=2` aus R ist in Python `interpolation='midpoint'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-3.33847407, -0.97765409,  3.92603856])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(residuals,[.25,.5,.75], interpolation='midpoint') # R type=2,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIC, BIC in Python Statsmodel\n",
    "\n",
    "Statsmodel hat eine andere Art das AIC zu berechnen. In `linear_model.py` ist die Funktion `aic()` definiert als\n",
    "\n",
    "```python\n",
    "    def aic(self):\n",
    "        return -2 * self.llf + 2 * (self.df_model + self.k_constant)\n",
    "```\n",
    "\n",
    "wobei `df.model` die Freiheitsgrade des Modells sind (also in dem Fall 1, da wir nur eine unabhängige Variable haben) und `k.constant` ist für die lineare Regression = 1. `llf` ist der LogLikelihood Wert und stimmt mit dem von R überein. Wir haben daher als Unterschied: \n",
    "\n",
    "$$AIC_R = -2\\cdot LL + 2 \\cdot 3$$\n",
    "$$AIC_{Py} = -2 \\cdot LL + 2 \\cdot 2$$\n",
    "\n",
    "Das `self.df_model` wird berechnet als\n",
    "\n",
    "```python\n",
    "self._df_model = float(self.rank - self.k_constant)\n",
    "```\n",
    "\n",
    "wobei hier `rank` der Rang der Matrix der Singulärwerte von $X$ ist, in unserem Fall = 2. Würde Python für das AIC statt `self.df_model` den Wert `df.rank` verwenden, hätte man das gleiche Ergebnis wie in R."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2844ba9b0855231960644b25734274ed565746943d06b8f800526790b893a63c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('bdsm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "2",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289.857px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}