{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "Random Forests gehören in die Familie der **Ensemble-Learner**. Hier wird nicht nur ein Klassifikator trainiert, sondern mehrere (ein **Ensemble**) und dann nach Mehrheitsentscheid ausgewählt (das funktioniert auch für die Regression).\n",
    "\n",
    "Bevor wir konkret Random Forests trainieren, sehen wir uns ein paar Grundlagen des Ensemble Learnings an. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "Mit dem `VotingClassifier` in sklearn kann man verschiedene Klassifikatoren gleichzeitig trainieren und über die Ergebnisse aggregieren. So bekommt man oft eine höhere Genauigkeit als bei den einzelnen Klassifikationen. Nimmt man beispielsweise die Mehrheitsentscheidung, spricht man von *hard voting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from bdsm.datasets import titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = titanic()\n",
    "df = df.to_numeric()\n",
    "df = df.drop(\"PassengerId\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Survived_cat\", axis=1)\n",
    "y = df[\"Survived_cat\"].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33,random_state=147)\n",
    "X_train = zscore(X_train)\n",
    "X_test = zscore(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "VotingClassifier(estimators=[('lr', LogisticRegression()),\n                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf),('rf',rf_clf),('svc',svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8169491525423729\n",
      "RandomForestClassifier 0.8372881355932204\n",
      "SVC 0.8372881355932204\n",
      "VotingClassifier 0.8372881355932204\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rf_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging und Pasting\n",
    "\n",
    "Ein anderer Ansatz als mehrere Klassifikatoren zu verwenden ist es, einen Klassifikator auf unterschiedlichen, zufällig erstellten Subsets der Trainingsdaten zu trainieren. Wird das Sample **mit** Zurücklegen erstellt, dann spricht man von **Bagging** (Abkürzung für *boostrap aggregating*), wird es **ohne** Zurücklegen erstellt nennt man es **Pasting**.\n",
    "\n",
    "In beiden Fällen werden die Trainingsinstanzen mehrmals von dem/den Klassifikator/en (es könnten auch wieder unterschiedliche sein) verwendet. Bei Bagging kann es sogar vorkommen, dass die gleiche Trainingsinstanz mehrmals verwendet wird (weil wir ja mit Zurücklegen samplen).\n",
    "\n",
    "Wenn alle Klassifikatoren trainiert sind, erstellt das Ensemble eine Vorhersage für einen neuen Datenpunkt, indem über alle Predictions der Klassifikatoren aggregiert wird. Dabei nimmt man üblicherweise \n",
    "\n",
    "- den **Modus** bei Klassifikationen\n",
    "- den **Mittelwert** bei Regressionen\n",
    "\n",
    "Jeder einzelne Prädiktor hat somit zwar einen höheren Bias, als wenn er auf den originalen Trainingsdaten trainiert wurde, insgesamt werden aber sowohl Bias als auch Varianz reduziert!\n",
    "\n",
    "Ein weiterer Vorteil ist, dass man alle Klassifikatoren **parallel** auf mehreren CPUs rechnen kann. Diese Skalierungseigenschaft macht das Bagging sehr attraktiv\n",
    "\n",
    "### Bagging Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n                  n_jobs=-1)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8305084745762712"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation\n",
    "\n",
    "Beim Bagging werden manche Instanzen der Trainingsdaten mehrmals ins Sample gewählt, manche gar nicht. Per default nimmt ein `BaggingClassifier` in sklearn $m$ Trainingsinstanzen mit Zurücklegen, wobei $m$ die Größe des Trainingssets ist. Es läßt sich zeigen, dass dadurch im Schnitt nur etwa 63% aller Trainingsinstanzen für jeden Klassifikator verwendet werden. Die verbleibenden 37%, die nicht verwendet werden, nennt man **out-of-bag** (oob)-Instanzen. Beachte, dass diese 37% für jeden Klassifikator unterschiedlich sein können.\n",
    "\n",
    "Da der Klassifikator diese oob Insanzen während des Trainings niemals sieht, sie aber trotzdem Teil des Trainingssets sind, bietet es sich an, diese als **Validation Set** zu betrachten.\n",
    "\n",
    "Um das durchzuführen, muss man nur die Option `oob_score=True` beim `BaggingClassifier` setzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.785234899328859"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleichen wir das nun mit der Accuracy auf den Testdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8372881355932204"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests in Python\n",
    "\n",
    "Statt einen `BaggingClassifier` und einen `DecisionTreeClassifier` zu kombinieren, kann man einfacher einen `RandomForestClassifier` verwenden, der für Decision Trees optimiert wurde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_leaf_nodes=16,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8372881355932204"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erklärung\n",
    "\n",
    "Der Random Forest hat noch ein zusätzliches Level an Randomness: anstatt nach dem besten Feature unter allen verfügbaren Features für einen Split zu suchen, betrachtet er nur ein **zufälliges** Subset an Features. Die Bäume werden so diverser, dadurch bekommt man zwar einen höheren Bias, aber eine geringere Varianz, was insgesamt zu einem besseren Modell führt!\n",
    "\n",
    "Durch die Ausführung als Ensemble-Methode verlieren wir die Erklärbarkeit, die wir bei einzelnen Bäumen hatten. Jedoch können wir uns die **Feature-Importance** ansehen, hier am Beispiel der Iris Daten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bdsm.datasets import iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = iris()\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sepal length 0.10208522594339844\n",
      "Sepal width 0.023296681514646655\n",
      "Petal length 0.44217568616312986\n",
      "Petal width 0.43244240637882503\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
    "rf_clf.fit(X, y)\n",
    "for name, score in zip(list(df.columns), rf_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2844ba9b0855231960644b25734274ed565746943d06b8f800526790b893a63c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('bdsm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}